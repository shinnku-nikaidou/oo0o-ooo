---
layout: "@/partials/BasePost.astro"
title: "Online Matching with Stochastic Rewards"
description: "Online Matching with Stochastic Rewards"
pubDate: 2025-10-23T00:00:00Z
imgSrc: "/img/posts/bipartite.png"
imgAlt: "A bipartite graph representation"
---

## Setting & model

### Online Bipartite Matching

The author Karp, Vazirani, and Vazirani introduce a bipartite graph model $G =(U \cup V, E)$,
where $U$ is given offline and $V$ arrives online [KVV90].
The online algorithm must irrevocably match each arriving vertex $v \in V$ to an unmatched neighbor in $U$ or leave it unmatched.

The offline optimal is defined as the maximum matching in $G$.
The competitive ratio is defined as the expected size of the matching produced by the online algorithm divided by the size of the offline optimal matching.

$$
Ratio = ALG / OPT
$$

### Online Stochastic Matching Problem

In the online matching problem with stochastic rewards, every edge $(u, v)$ also has an associated probability of success $p_{uv}$.

When a new vertex $v \in V$ arrives online, the algorithm either chooses not to assign it at all, or assigns it to one of its neighbors (these are the available options).

If $v$ is assigned to $u \in U$ using the edge $(u, v)$, a Bernoulli experiment is performed and the edge $(u, v)$ becomes a successful assignment with probability $p_{uv}$.

In successful assignment case, we say that $u$ was successful and remove it from the set of _available_ vertices.
On the other hand, if the assignment $(u, v)$ is not successful, $u$ remains available and a neighbor of $u$ that arrives later in the online order can be assigned to $u$ (but $v$ cannot be assigned in the future).

The overall objective is to maximize the _expected_ number of successful vertices $u \in U$, or equivalently, the expected number of successful assignments.

### Vertex-Weighted Stochastic Matching

In the vertex-weighted variant, each offline vertex $u \in U$ has a nonnegative weight $w_u$.
A successful assignment on $u$ yields reward $w_u$ and removes $u$ from availability (as before).
The objective becomes maximizing the **expected total weight** of successful offline vertices:

$$
\max  \mathbb{E}\!\left[\sum_{u \in U} w_u \cdot \mathbf{1}\{u\ \text{is matched successfully}\}\right].
$$

The unweighted model is the special case $w_u \equiv 1$.

The four probability regimes below (Vanishing/Arbitrary × Equal/Unequal) remain the same; results in recent work typically extend to the vertex-weighted case with essentially the same analyses.

### Probability Settings

1. Vanishing & Equal Probabilities
2. Vanishing & Unequal Probabilities
3. Arbitrary/Non-Vanishing & Equal Probabilities
4. Arbitrary/Non-Vanishing & Unequal

## Offline Optimal Benchmark

### The Budgeted Allocation Problem

Let $G = (U \cup V, E)$ be a bipartite graph where edge $(u, v)$ has weight
$p_{uv}$.
For every vertex $v \in V$, we assign it to one its neighbors $u \in U$ using
edge $(u, v)$.
The _load_ $L_u$ on a vertex $u \in U$ is defined as the sum of weights of
assigned edges incident on $u$.
The objective is to maximize $\sum_{u \in U} \min(L_u, 1)$.

For technical reasons, we allow fractional solutions, i.e. vertex $v \in V$
can be assigned to its neighbors $u \in U$ with fractions $x_{uv}$ subject to
$\sum_{u \in U} x_{uv} \le 1$; then $L_u = \sum_{u \in U} x_{uv} p_{uv}$.

For any instance of the **Online Stochastic Matching** problem, we define
$OPT$ to be the optimum fractional solution for the corresponding **Budgeted
Allocation** problem.
The next lemma claims that the value of $OPT$ is at least the expected number
of successes obtained by _any_ **Online Stochastic Matching** algorithm.

## Distribution of the Success Threshold (Ө)

Let $\tau$ be the round of the first success when repeatedly attempting the same offline vertex $u$. Then

$$
\tau \sim \operatorname{Geom}(p) \quad(\tau=1,2,3, \ldots) .
$$

Measure "load" as the cumulative success probability: each attempt adds $p$. Define

$$
\Theta:=p \cdot \tau .
$$

Hence,

$$
\operatorname{Pr}[\Theta=t p]=p(1-p)^{t-1} \quad(t=1,2, \ldots)
$$

so $\Theta$ only takes values in $\{p, 2 p, 3 p, \ldots\}$.
A vertex $u$ becomes successful the first time its accumulated load $\ell_u$
reaches or exceeds $\Theta$.
This is exactly the "equivalent threshold view" used by Mehta-Panigrahi.

Basic moments. $\mathbb{E}[\Theta]=1$ and $\operatorname{Var}(\Theta)=1-p$.
Thus, as $p \rightarrow 0$, both the mean and the variance tend to 1.

Vanishing limit $(p \rightarrow 0)$.
Viewing the discrete distribution as a continuous-load approximation,

$$
\operatorname{Pr}[\Theta>x]=(1-p)^{\lfloor x / p\rfloor} \underset{p \rightarrow 0}{\longrightarrow} e^{-x}
$$

i.e., $\Theta \Rightarrow \operatorname{Exp}(1)$.
Consequently, in small-$p$ analyses it is standard to model each offline
vertex's threshold as an independent $\operatorname{Exp}(1)$ random variable.

Unequal but small probabilities.
If the edge success probabilities $p_{u v}$ differ but are all small, we still
measure load as the sum of allocated success probabilities.
In this continuous-load coordinate, the threshold is well-approximated by
$\operatorname{Exp}(1)$; each attempt just increments the load by the
corresponding $p_{u v}$ instead of a fixed $p$.

## Definition of Competitive Ratio

Let $N_u$ denote the set of neighbors of an offline vertex $u$.

For the **unweighted** case, the competitive ratio is $\mathbb{E}[ALG]/OPT$.

For the **vertex-weighted** case, the competitive ratio is $\mathbb{E}[ALG_w]/OPT_w$.

A convenient upper bound/benchmark is the (configuration) LP relaxation; a simple edge-based LP that suffices for exposition is:

$$
\begin{aligned}
\max \quad & \sum_{u \in U} w_u\cdot p_{uv} \cdot x_{uv} \\
\text{s.t.}\quad
& \sum_{v \in N(u)} p_{uv}\, x_{uv} \le 1 \qquad && \forall u \in U && (1) \\
& \sum_{u \in N(v)} x_{uv} \le 1 \qquad && \forall v \in V && (2) \\
& 0 \le x_{uv}
\end{aligned}
$$

Here $x_{uv}$ is the (fractional) probability of attempting $(u,v)$, all
$w_u\equiv 1$ recovers the unweighted benchmark.

Eqn. (1) states that the expected load of each offline vertex $u$ is upper
bounded by 1, because it is upper bounded by the expectation of the threshold
$\theta_u$, which equals 1. (this uses the Law of Total Probability)

Eqn. (2) is a standard capacity constraint: each online vertex $v$ can be
matched to at most one offline neighbor.

Following the vertex-weighted model of Mehta and Panigrahi [24], we will
consider the optimal objective of the above LP relaxation as the benchmark.

**Configuration LP**

For any $S \subseteq N_u$, let $p_u(S)= \min \left\{\sum_{v \in S} p_{u v}, 1\right\}$.
Consider the following offline-side configuration LP:

$$
\begin{aligned}
\text{ConfigLP:} \quad \max \quad & \sum_{u \in U} \sum_{S \subseteq N_u} w_u \cdot p_u(S) \cdot x_{u S} \\
\text{s.t.} \quad & \sum_{S \subseteq N_u} x_{u S} \leq 1 && \forall u \in U \\
& \sum_{u \in U} \sum_{S \subseteq N_u: v \in S} x_{u S} \leq 1 && \forall v \in V \\
& x_{u S} \geq 0 && \forall u \in U, \forall S \subseteq N_u
\end{aligned}
$$

From an offline optimization viewpoint, as $p$, the upper bound of success
probabilities, tends to zero, the configuration LP is equivalent to the
standard matching LP in the following sense.
First, the matching LP can be reinterpreted as to maximize

$$
\sum_{u \in U} w_u \cdot \min \left\{\sum_{v:(u, v) \in E} p_{u v} x_{u v}, 1\right\}
$$

subject to only Eqn. (2) and (3).

On one hand, any feasible assignment of the configuration LP corresponds to a
feasible assignment of the matching LP with

$$
x_{u v}=\sum_{S \in N_u: v \in S} x_{u S}.
$$

Comparing the objectives of the LPs, the matching LP objective is weakly
larger because $\min \{z, 1\}$ is concave.

On the other hand, given any feasible assignment of the matching LP, we can
sample an integral matching via independent rounding, i.e., match each online
vertex $v$ to a neighbor that is independently sampled according to
$x_{u v}$ 's.

Then, it gives an integral assignment of the configuration LP: $x_{u S}=1$ for
the subset $S$ of neighbors matched to $u$ in the independent rounding, and 0
otherwise.
Further, by standard concentration inequalities (and union bound), the load of
each vertex $u$ is at most its expected load, which is at most 1 due to
Eqn. (1), plus an additive error that diminishes as $p$ tends to zero.
Hence, the optimal of the configuration LP is at least that of the matching LP
less a term that diminishes as $p$ tends to zero.

The corresponding dual LP of the configuration LP is the following:

$$
\begin{aligned}
\text{Dual:} \quad \min \quad & \sum_{u \in U} \alpha_u+\sum_{v \in V} \beta_v \\
\text{s.t.} \quad & \alpha_u+\sum_{v \in S} \beta_v \geq w_u \cdot p_u(S) && \forall u \in U, \forall S \subseteq N_u \\
& \alpha_u, \beta_v \geq 0 && \forall u \in U, \forall v \in V
\end{aligned}
$$

An algorithm is $\Gamma$-competitive if:

1. Equal primal and dual objectives:

$$
\sum_{u \in U} \sum_{S \subseteq N_u} p_u(S) \cdot w_u \cdot x_{u S}=\sum_{u \in U} \alpha_u+\sum_{v \in V} \beta_v ;
$$

Approximate dual feasibility: For any $u \in U$ and any $S \subseteq N_u$,

$$
\mathbf{E}\left[\alpha_u+\sum_{v \in S} \beta_v\right] \geq \Gamma \cdot w_u \cdot p_u(S)
$$

## Analyses

Let $\ell_u$ denotes the load of an offline vertex $u \in U$, $\Theta$ be the vector of $\Theta_u$ for all $u \in U$, and let $\Theta_{-u}$ denote the entire vector $\Theta$ except $\Theta_u$.
For any fixed ( $n-1$ )-dimensional vector $\theta$, let

$$
p_u(\theta):=\operatorname{Pr}\left[\Theta_{-u}=\theta\right]
$$

Further, let $\ell_u^{\infty}(\theta)$ be the load on vertex $u$ when $\Theta_{-u}=\theta$ and $\Theta_u=\infty$; correspondingly, let

$$
q_u(x):=\sum_{\substack{\theta: \ell_u^{\infty}(\theta)=x }} p_u(\theta)
$$

### Dual Assignment

Note that $u$ 's load increases over time; let it be the current load in the following discussion.
For some **non-decreasing function** $f: \mathbf{R}^{+} \mapsto \mathbf{R}^{+}$to be determined in the analysis, we maintain the dual assignment as follows.

1. Initially, let $\alpha_u=0$ for all $u \in U$.
2. On the arrival of an online vertex $v \in V$ and, thus, the corresponding dual variable $\beta_v$ :
   (a) If $v$ is matched to $u \in U$, increase $\alpha_u$ by $p f\left(\ell_u\right)$ and let $\beta_v=p\left(1-f\left(\ell_u\right)\right)$.
   (b) If $v$ has no unsuccessful neighbor, let $\beta_v=0$.

Let $F(\ell)=\int_0^{\ell} f(z) d z$ denote the integral of $f$. Note that we have $\alpha_u=F\left(\ell_u\right)$ in the limit as $p$ tends to zero.

**Contribution from $ \alpha_u $.**  
We now proceed to prove approximate dual feasibility using the above characterization. First, consider the contribution from the offline vertex $u$.

_For any thresholds $ \theta\_{-u} $ of offline vertices other than $u$ and the corresponding $ \ell_u^{\infty} $:_

$$
\mathbb{E}_{\theta_u}\!\left[\alpha_u \mid \theta_{-u}\right]
\ge
\int_{0}^{\ell_u^{\infty}} e^{-\theta_u}\, f(\theta_u)\, d\theta_u \, .
$$

By the above characterization, $u$'s load equals $ \ell_u^{\infty} $ when $ \theta_u \ge \ell_u^{\infty} $, and equals $ \theta_u $ when $ 0 \le \theta_u < \ell_u^{\infty} $. Further, recall that $ \alpha_u = F(\ell_u) $ and that $ \theta_u \sim \operatorname{Exp}(1) $. We get that:

$$
\mathbb{E}_{\theta_u}\!\left[\alpha_u \mid \theta_{-u}\right]
\ge
\int_{0}^{\ell_u^{\infty}} e^{-\theta_u} F(\theta_u)\, d\theta_u
+
e^{-\ell_u^{\infty}} F(\ell_u^{\infty})
=
\int_{0}^{\ell_u^{\infty}} e^{-\theta_u} f(\theta_u)\, d\theta_u \, .
$$

Here, the equality follows by integration by parts. $\square$

---

**Contribution from $ \beta_v $’s.**  
Next, consider the contribution from the online vertices $ v \in S $. For ease of notations, we let $ z^{+} $ denote $ \max\{z,0\} $.

For any thresholds $ \theta\_{-u} $ of offline vertices other than $u$ and the corresponding $ \ell*u^{\infty} $:*

$$
\begin{aligned}
\mathbb{E}_{\theta_u}\!\left[\sum_{v\in S} \beta_v \,\middle|\, \theta_{-u}\right]
\ge
&\left(
e^{-\ell_u^{\infty}}\, p_u(S)
+
\int_{0}^{\ell_u^{\infty}} e^{-\theta_u}\, \big( p_u(S) - (\ell_u^{\infty}-\theta_u)^{+} \big)\, d\theta_u
\right) \\
&\times \big(1 - f(\ell_u^{\infty})\big)\, .
\end{aligned}
$$

_Proof._ By the above characterization, all vertices $ v \in S $ are matched to neighbors with load at most $ \ell_u^{\infty} $ at the time of the matches when $ \theta_u \ge \ell_u^{\infty} $. This happens with probability $ e^{-\ell_u^{\infty}} $ and contributes:

$$
e^{-\ell_u^{\infty}}\, p_u(S)\cdot \big(1 - f(\ell_u^{\infty})\big),
$$

to the expectation in the lemma.

Similarly, all vertices $ v \in S $, except for $ p^{-1}(\ell_u^{\infty}-\theta_u) $ of them, are still matched to neighbors with load at most $ \ell_u^{\infty} $ at the time of the matches when $ 0 \le \theta_u < \ell_u^{\infty} $.

> In the Stochastic Balance algorithm, each online vertex is matched to the unsuccessful neighbor with the least load, so only the overflow load $ \ell_u^{\infty}-\theta_u $ exceeds $ \theta_u $ and causes $ u $ to become successful.

This contributes:

$$
\int_{0}^{\ell_u^{\infty}} e^{-\theta_u}\, \big( p_u(S) - (\ell_u^{\infty}-\theta_u)^{+} \big)\, d\theta_u \cdot \big(1 - f(\ell_u^{\infty})\big),
$$

to the expectation. $\square$

So we have:

$$
\mathbf{E}\left[\alpha_u+\sum_{v \in S} \beta_v\right] \geq \int_0^{\ell_u^{\infty}} e^{-\theta_u} f\left(\theta_u\right) d \theta_u+\left(e^{-\ell_u^{\infty}} p_u(S)+\int_0^{\ell_u^{\infty}} e^{-\theta_u}\left(p_u(S)-\left(\ell_u^{\infty}-\theta_u\right)\right)^{+} d \theta_u\right)\left(1-\mathscr{f}\left(\ell_u^{\infty}\right)\right)
$$

To show approximate dual feasibility, it suffices to find a non-decreasing function $f$ such that for any $\ell_u^{\infty} \geq 0$ and any $0 \leq p_u(S) \leq 1$ :

$$
\int_0^{\ell_u^{\infty}} e^{-\theta_u} f\left(\theta_u\right) d \theta_u+\left(e^{-\ell_u^{\infty}} p_u(S)+\int_0^{\ell_u^{\infty}} e^{-\theta_u}\left(p_u(S)-\left(\ell_u^{\infty}-\theta_u\right)\right)^{+} d \theta_u\right)\left(1-f\left(\ell_u^{\infty}\right)\right) \geq \Gamma \cdot p_u(S)
$$

Therefore, there is at most one index $j$ such that $\ell_j<\ell_u^{\infty}$ and $\ell_{j+1} \geq \ell_u^{\infty}$.

### Solving the Differential Inequality

For ease of notations, we write $\ell_u^{\infty}$ as $\ell, p_u(S)$ as $p$, and $\theta_u$ as $\theta$. Then, =becomes:

$$
\int_0^{\ell} e^{-\theta} f(\theta) d \theta+\left(e^{-\ell} p+\int_0^{\ell} e^{-\theta}(p-(\ell-\theta))^{+} d \theta\right)(1-f(\ell)) \geq \Gamma \cdot p .
$$

First, note that the $z^{+}$operation effectively restricts the second integration in the above equation to be from max $\{0, \ell-p\}$ to $\ell$. This allows us to simplify the above equation by taking away the $z^{+}$operation. Concretely, it suffices to ensure that for any $0 \leq p<\ell$ :

$$
\int_0^{\ell} e^{-\theta} f(\theta) d \theta+(1-f(\ell))\left(e^{-\ell+p}-e^{-\ell}\right) \geq \Gamma \cdot p,
$$

and for any $0 \leq \ell \leq p$ :

$$
\int_0^{\ell} e^{-\theta} f(\theta) d \theta+(1-f(\ell))\left(1-\ell+p-e^{-\ell}\right) \geq \Gamma \cdot p .
$$

Then, we show that the worst-case scenarios are when there is a unit mass of online neighbors, i.e., when $p = 1$.

$$
1-f(\ell) \leq 1-f(0) \leq \Gamma
$$

for Eqn. (7). So the lemma follows.
Then it suffices to solve the following simplified differential inequalities subject to the boundary condition that $f(0)=1-\Gamma$. For any $\ell>1$, we need:

$$
\int_0^{\ell} e^{-\theta} f(\theta) d \theta+(1-f(\ell))\left(e^{-\ell+1}-e^{-\ell}\right) \geq \Gamma
$$

while for any $0 \leq \ell \leq 1$, we need:

$$
\int_0^{\ell} e^{-\theta} f(\theta) d \theta+(1-f(\ell))\left(2-\ell-e^{-\ell}\right) \geq \Gamma
$$

Right Boundary Condition. To further simplification, we impose a right boundary condition that it suffices to ensure that $f(\ell)=1-\frac{1}{e}$ for $\ell \geq 1$.

Lemma 11. Suppose $f:[0,1] \mapsto \mathbf{R}^{+}$is a non-decreasing function such that $f(0)=1-\Gamma, f(1)= 1-\frac{1}{e}$, and Eqn. (9) holds for any $0 \leq \ell \leq 1$. Then, the extension of $f$ such that $f(\ell)=f(1)=1-\frac{1}{e}$ for all $\ell>1$ satisfies Eqn. (4) for all $\ell \geq 0$.

Proof. By the assumption of $f$, it suffices to show that the difference between the RHS of Eqn. (8) when with any $\ell>1$ and that when $\ell=1$ is non-negative. The difference is (recalling that $f(\ell)=1-1 / e$ for all $\ell \geq 1$ ):

$$
\begin{aligned}
& \left(\int_0^{\ell} e^{-\theta} f(\theta) d \theta+(1-f(\ell))\left(e^{-\ell+1}-e^{-\ell}\right)\right)-\left(\int_0^1 e^{-\theta} f(\theta) d \theta+(1-f(1))\left(1-e^{-1}\right)\right) \\
& \quad=\int_1^{\ell} e^{-\theta} f(1) d \theta+(1-f(1))\left(\left(e^{-\ell+1}-e^{-\ell}\right)-\left(1-e^{-1}\right)\right) \\
& \quad=\left(1-e^{-\ell+1}\right)\left(f(1)-\left(1-e^{-1}\right)\right) \\
& \quad=0
\end{aligned}
$$

As a result, it suffices to find a non-decreasing function $f$ subject to the boulhdary conditions $f(0)=1-\Gamma$ and $f(1)=1-1 / e$, such that Eqn.(9) holds for any $0 \leq \ell \leq 1$. Solving this differential inequality for $f$ gives,

$$
f(x)=\frac{1}{h(x)}\left(1-\frac{1}{e}+\int_x^1\left(1-e^{-y}\right) g(y) h(y) d y\right)
$$

where $g(x)=\frac{1}{2-x-e^{-x}}$, and $h(x)=\exp \left(\int_x^1 g(z) d z\right)$.
To conclude, we find the $f(x)$ shown in Eqn.(5). Thus $\Gamma=1-f(0) \approx 0.576$.

---

## Worst case scenario analysis

### Online Matching on the Z-shaped Bipartite Graph — Full Derivation (Random Seat Selection)

This note gives a complete, self-contained derivation for the model:
Each online node $v_j$ (where $j\in \{1,\dots,n\}$) draws $N_j\sim\mathrm{Pois}(1)$ and then picks
$\min(N_j,\#\text{AvailableNeighbors})$ **uniformly at random without replacement**
from its currently available neighbors. We analyze the expected total matches on the
triangular ("Z-shaped") bipartite graph with the $G(n,k)$ structure.

#### Model

- Offline nodes: $u_1, u_2, \dots, u_n$, each with capacity 1.
- Online nodes: $v_1, v_2, \dots, v_n$, arriving in order.
- Adjacency:
  - If $j\le k$: online node $v_j$ connects to offline nodes $\{u_j, u_{j+1}, \dots, u_n\}$.
  - If $j>k$: online node $v_j$ connects only to offline node $u_j$ (diagonal).
- Processing rule:
  - Independently draw $N_j\sim\mathrm{Pois}(1)$.
  - At the moment online node $v_j$ is processed, let $S_j$ be the set of its currently available
    neighbors and $R_j \overset{\text{def}}{=} |S_j|$ .
  - Pick $Y_j=\min(N_j,R_j)$ offline nodes **uniformly at random without replacement** in $S_j$
    and occupy them.

Let $M_{n,k}$ be the total number of matched offline nodes when all $n$ online nodes have been processed.

#### Phase Split

We split the process into two phases:

- **Phase A:** online nodes $v_j$ for $j=1,\dots,k$ (suffix online nodes).
- **Phase B:** online nodes $v_j$ for $j=k+1,\dots,n$ (diagonal online nodes).

Total matches:

$$
M_{n,k} = M^{(A)} + M^{(B)}.
$$

#### Phase A — Per-online-node "hit probability" (discrete hazard)

Fix an online node $v_j$ with $j\le k$. Conditional on the history (all randomness before online node $v_j$ is processed),
the set $S_j$ of available neighbors has size $R_j$.
By symmetry of **uniform without replacement** within $S_j$,
each offline node $u_i\in S_j$ has the **same** conditional hit probability

$$
h_j
=
\Pr(\text{online node }v_j\text{ selects a given }u_i\in S_j \mid S_j)
=
\frac{\mathbb{E}[\,\min(N_j,R_j)\mid S_j]}{R_j}.
$$

> With-replacement "Poissonization" (often used for occupancy):

$$
\tilde h_j = 1-\mathbb{E}\!\left[\Big(1-\tfrac{1}{R_j}\Big)^{N_j}\Bigm|S_j\right]
= 1-e^{-1/R_j}
= \tfrac{1}{R_j}+O(R_j^{-2}).
$$

> For large $R_j$ the without/with-replacement difference is $O(R_j^{-2})$, which is negligible
> in our large-$n$ limit (it contributes only $o(1)$ after summing across online nodes).

**Key single-online-node conservation:**
the total hit probability online node $v_j$ "sprays" across its available set is

$$
\sum_{u_i\in S_j} h_j
=
R_j\cdot h_j
=
\mathbb{E}[\min(N_j,R_j)\mid S_j]
=
1-\Pr(N_j\ge R_j\mid S_j)
\approx1
\quad(\text{since typically }R_j\gg 1).
$$

#### Phase A — Mass Transport (double counting / conservation of expectation)

Let $U$ be a uniformly random offline node in $\{u_1,\dots,u_n\}$, sampled independently of the process.
Define the **cumulative Phase-A hazard on offline node $U$** by

$$
H_U^{(A)} \overset{\text{def}}{=} \sum_{j=1}^{k} h_j\,\mathbf{1}\{\,U\in S_j\,\}.
$$

Taking expectations and swapping sums (double counting):

$$
\sum_{i=1}^n \mathbb{E}[H_{u_i}^{(A)}]
=
\sum_{j=1}^{k} \mathbb{E}\!\left[\sum_{u_i\in S_j} h_j\right]
=
\sum_{j=1}^{k} \mathbb{E}[\min(N_j,R_j)].
$$

Averaging over offline nodes:

$$
\boxed{
\mathbb{E}[H_U^{(A)}]
=
\frac{1}{n}\sum_{j=1}^{k} \mathbb{E}[\min(N_j,R_j)]
=
\frac{k}{n} + o(1).
}
$$

Intuition: each of the first $k$ online nodes contributes about 1 unit of "risk" across offline nodes,
so per offline node the **average** Phase-A hazard is $k/n$ (up to $o(1)$).

> **Non-uniformity note.** For a _fixed_ offline node $u_i$,
> only online nodes $v_j$ with $j\le \min\{i,k\}$ can hit it, so its mean Phase-A hazard is
> $\mu_i\approx \min\{i,k\}/n$ (not uniform). We will use this refined view below.

#### Phase A — Survival probability and expected matches

Conditional on the process, the probability that offline node $U$ survives Phase A (remains empty) is

$$
\prod_{j=1}^{k}\bigl(1-h_j\,\mathbf{1}\{U\in S_j\}\bigr).
$$

When each $h_j$ is small (here $h_j=O(1/R_j)=O(1/n)$ for almost all rows),

$$
\prod_{j}(1-h_j)=\exp\!\Big(-\sum_j h_j + O(\sum_j h_j^2)\Big)
\approx e^{-H_U^{(A)}},
$$

and $\sum_j h_j^2=o(1)$. Concentration (bounded differences / Azuma–McDiarmid) further pins
$H_U^{(A)}$ near its mean, hence

$$
\mathbb{E}[\text{Phase-A survival of }U]
=\mathbb{E}[e^{-H_U^{(A)}}]
=e^{-\mathbb{E}[H_U^{(A)}]+o(1)}
=e^{-k/n+o(1)}.
$$

Therefore,

$$
\boxed{
\mathbb{E}[M^{(A)}]
= n-n\,e^{-k/n} + o(n).
}
$$

> **Refined (non-uniform) Phase-A sum.** If you want the offline-node-by-offline-node form,

$$
\mathbb{E}[M^{(A)}]
\approx
\sum_{i=1}^{k}\bigl(1-e^{-i/n}\bigr)+
\sum_{i=k+1}^{n}\bigl(1-e^{-k/n}\bigr)
=
n - \underbrace{\sum_{i=1}^{k} e^{-i/n}}_{\text{left part}} - (n-k)e^{-k/n}.
$$

Since $\sum_{i=1}^{k} e^{-i/n} = e^{-1/n}\frac{1-e^{-k/n}}{1-e^{-1/n}} = k - \frac{k(k-1)}{2n}+O(n^{-2})$,
this reduces to $n - n e^{-k/n} + O(1)$, consistent with the coarser $n - n e^{-k/n}$.

#### Phase B — Diagonal online nodes

For each $j>k$, online node $v_j$ connects only to offline node $u_j$.
Let $E_j$ be the event that offline node $u_j$ is still empty after Phase A.

- $\Pr(E_j)\approx e^{-k/n}$ (same hazard/survival logic as above, specialized to the group $i>k$).
- Independently, $N_j\sim\mathrm{Pois}(1)$, so online node $v_j$ succeeds on offline node $u_j$ with probability $\Pr(N_j\ge 1)=1-e^{-1}$.

Hence

$$
\boxed{
\mathbb{E}[M^{(B)}]
\approx
(n-k)\,e^{-k/n}\,(1-e^{-1}).
}
$$

#### Combine Phases A and B

$$
\begin{aligned}
\mathbb{E}[M_{n,k}]
&= \mathbb{E}[M^{(A)}] + \mathbb{E}[M^{(B)}] \\
&\approx \bigl(n - n e^{-k/n}\bigr) + \bigl((n-k)e^{-k/n}(1-e^{-1})\bigr) \\
&= n-e^{-k/n}\,\Bigl(k + (n-k)e^{-1}\Bigr).
\end{aligned}
$$

Equivalently, with $\alpha=k/n$,

$$
\boxed{
\frac{\mathbb{E}[M_{n,k}]}{n}
\approx
1 - e^{-\alpha}\,\bigl(\alpha + (1-\alpha)e^{-1}\bigr),\quad \alpha\in[0,1].
}
$$

**Sanity checks**

- $k=0$ (all diagonal): $\mathbb{E}[M]/n\approx 1-e^{-1}$.
- $k=n$ (all suffix): $\mathbb{E}[M]/n\approx 1-e^{-1}$ — matches simulations ($\approx 0.63212$).
- $0<k<n$: slightly **below** $1-e^{-1}$ (a shallow bowl).

#### Optimizing over $\alpha=k/n$

Let

$$
f(\alpha)
=
1 - e^{-\alpha}\,\bigl(e^{-1} + \alpha(1-e^{-1})\bigr),
\qquad
g(\alpha)=1-f(\alpha)=e^{-\alpha}\,\bigl(c+\alpha(1-c)\bigr),
\quad c=e^{-1}.
$$

Then

$$
g'(\alpha)=e^{-\alpha}\,\bigl((1-2c)-(1-c)\alpha\bigr)=0
\Longrightarrow
\boxed{
\alpha^*=\frac{1-2c}{1-c}=\frac{e-2}{\,e-1\,}=1-\frac{1}{e-1}\approx 0.418023.
}
$$

At $\alpha^*$, $g$ is maximized (so $f$ is minimized). The minimum value is

$$
\boxed{
f_{\min}
=1-(1-e^{-1})\,e^{-\alpha^*}
=1-(1-e^{-1})\,\exp\!\Bigl(-\frac{e-2}{e-1}\Bigr)
\approx 0.583845.
}
$$

#### Notes on accuracy and concentration

- **With vs. without replacement:** the difference in a single online node is $O(R_j^{-2})$,
  and across $k=O(n)$ online nodes the cumulative effect is $o(1)$ in normalized expectation.

- **Concentration:** Why can we replace $\mathbb{E}[e^{-H_U^{(A)}}]$ with $e^{-\mathbb{E}[H_U^{(A)}]}$?

  The key is that $H_U^{(A)} = \sum_{j=1}^{k} h_j \mathbf{1}\{U \in S_j\}$ concentrates tightly around its mean.

  **Bounded differences property.** When online node $v_j$ arrives:
  - Suppose $R_j \geq n - j + 1$ (almost always true in Phase A)
  - The discrete hazard $h_j \approx 1/R_j$ is the probability that $v_j$ hits any specific offline node in $S_j$
  - Changing $v_j$'s random choice affects $H_U^{(A)}$ by at most $1/R_j$ (it can only "hit" $U$ with probability $\approx 1/R_j$)
  - After summing across $N_j \leq L = O(\log n)$ attempts (since $N_j \sim \mathrm{Pois}(1)$ is typically small), we get Lipschitz constant:
    $$
    c_j \leq \frac{CL}{n-j+1} \leq \frac{CL}{n(1-\alpha)}, \quad \text{where } L = O(\log n)
    $$

  **Applying Azuma–McDiarmid.** Define $M_j = \mathbb{E}[H_U^{(A)} \mid v_1,\dots,v_j \text{ processed}]$ as a Doob martingale. The bounded differences give:

  $$
  \sum_{j=1}^k c_j^2
  \leq
  \frac{C^2L^2}{n^2} \sum_{j=1}^k \frac{1}{(1-\frac{j-1}{n})^2}
  = O\left(\frac{(\log n)^2}{n}\right)
  $$

  By the Azuma–McDiarmid inequality (for martingales with bounded differences), for any $\varepsilon > 0$:

  $$
  \Pr\bigl(|H_U^{(A)} - \mathbb{E}[H_U^{(A)}]| > \varepsilon\bigr)
  \leq
  2\exp\left(-\frac{2\varepsilon^2}{\sum_{j=1}^k c_j^2}\right)
  =
  2\exp\left(-\Theta\left(\frac{\varepsilon^2 n}{(\log n)^2}\right)\right)
  $$

  Setting $\varepsilon = \Theta(n^{-A})$ for any $A > 0$ (say $A = 1/2$), the deviation probability is exponentially small. Therefore $H_U^{(A)} = \mathbb{E}[H_U^{(A)}] + \tilde{O}(1/\sqrt{n})$ with high probability, which justifies:

  $$
  \mathbb{E}[e^{-H_U^{(A)}}]
  =
  e^{-\mathbb{E}[H_U^{(A)}] + o(1)}
  =
  e^{-k/n + o(1)}
  $$

- **Refined Phase-A formula** using $\Pr(\text{survive at }u_i)\approx e^{-\min\{i,k\}/n}$
  yields
  $$
  \mathbb{E}[M^{(A)}]
  \approx \sum_{i=1}^{k}\bigl(1-e^{-i/n}\bigr) + (n-k)\bigl(1-e^{-k/n}\bigr)
  = n - n e^{-k/n} + O(1),
  $$
  consistent with the coarse expression.

#### TL;DR

For the **random selection** mechanism,

$$
\boxed{
\mathbb{E}[M_{n,k}] \approx n - e^{-k/n}\,\bigl(k + (n-k)e^{-1}\bigr), \quad
\frac{\mathbb{E}[M_{n,k}]}{n}
\approx
1 - e^{-\alpha}\bigl(\alpha+(1-\alpha)e^{-1}\bigr).
}
$$

The ratio is minimized at

$$
\boxed{\alpha^*=\dfrac{e-2}{e-1}\approx 0.418,\quad f_{\min}\approx 0.583845,}
$$

while the extreme cases $k=0$ and $k=n$ both give $\mathbb{E}[M]/n\approx 1-e^{-1}$.
